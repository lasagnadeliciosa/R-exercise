describe(times.rank)
#ties.method = c("average", "first", "random", "max", "min")
times.rankr <- rank(sn$Times, ties.method = "random")
hist(times.rankr)
decribe(times.rankr)
describe(times.rankr)
time.gt1 <- ifelse(sn$Times > 1, 1, 0)
time.gt1
rm(all)
rm("all")
time.gt1 <- ifelse(sn$Times > 1, 1, 0) #dichotomizing
n1 <- rnorm(1000000)
hist(n1)
n2 <- rnorm(1000000)
hist(n2)
n.add <- n1+n2 #average scores across two variables
hist(n.add)
n.mult <- n1*n2 #multiple scores across two vatiables
hist(n.mult)
install.packages("psyche")
install.packages("psych")
library("psych")
#calculate kurtosis for each distribution
kurtosi(n1)
kurtosi(n2)
kurtosi(n.add)
kurtosi(n.mult)
google <- read.csv("google_correlate.csv", header = T)
names(google)
str(google)
viz.reg.dist <- split(google$data_viz, google$region)
boxplot(viz.reg.dist, col = "lavender")
#barplot with means
viz.reg.mean <- sapply(viz.reg.dist, mean)
barplot(viz.reg.mean,
col = "beige",
main =
barplot(viz.reg.mean,
col = "beige",
main = "Average Google Searchh Share of\n\"Data Visualization\" by Region of US")
barplot(viz.reg.mean,
col = "beige",
main = "Average Google Searchh Share of\n\"Data Visualization\" by Region of US")
viz.reg.mean <- sapply(viz.reg.dist, mean) #s-apply
barplot(viz.reg.mean,
col = "beige",
main = "Average Google Search Share of\n\"Data Visualization\" by Region of US")
viz.reg.mean <- sapply(viz.reg.dist, mean) #s-apply
barplot(viz.reg.mean,
col = "beige",
main = "Average Google Search Share of\n\"Data Visualization\" by Region of US")
abline(h = 0)
library("psych", lib.loc="~/R/win-library/3.4")
install.packages("psych")
install.packages("psych")
install.packages("psych")
library("psych")
describeBy(google$data_viz, google$region)
View(google)
rm(viz.reg.dist, viz.reg.mean)
rm(viz.reg.dist, viz.reg.mean, google)
google <- read.csv("google_correlate.csv", header = T)
names(google)
str(google)
plot(google$degree, google$data_viz)
plot(google$degree, google$data_viz,
main = "Interest in Data Visualization Searches\nby Percentage",
xlab = "Population with College Degrees",
ylab = "Searches for \"Data Visualization\"",
pch = 20,
col= "grey")
plot(google$degree, google$data_viz,
main = "Interest in Data Visualization Searches\nby Percentage of Pupulation with College Degrees",
xlab = "Population with College Degrees",
ylab = "Searches for \"Data Visualization\"",
pch = 20, # appearance of the points
col= "grey")
#add Linear regression line
abline(lm(google$data_viz ~ google$degree, col = "red"))
#add Linear regression line
abline(lm(google$data_viz ~ google$degree, col="red"))#lm for linear model
#add Linear regression line
abline(lm(google$data_viz ~ google$degree, col="red"))#lm for linear model
#add Linear regression line
abline(lm(google$data_viz ~ google$degree, col="red"))#lm for linear model
#add Linear regression line
abline(lm(google$data_viz ~ google$degree), col="red")#lm for linear model
# Lowess smoother line
lines(lowess(google$degree ~ google$data_viz), col="blue")
# Lowess smoother line
lines(lowess(google$degree ~ google$data_viz), col="blue")
# Lowess smoother line
lines(lowess(google$degree, google$data_viz), col="blue")
rm(google)
google <- read.csv("google_correlate.csv", header = T)
names(google)
pairs(~data_viz + degree + facebook + nba,
data = google,
pch = 20,
main = "Simple Scatterplot Matriz")
#Use "Pairs Plot" from "psych" package
install.packages("psych")
install.packages("psych")
library("psych")
pairs.panels(google[c(3,7,4,5)], gap = 0)
rm(google)
google <- read.csv("google_correlate.csv", header = T)
#Spinning 3D scatterplot
#Install and load rgl package
install.packages("rgl")
library("rgl")
plot3d(google$data_viz, #X variable
google$degree,   #Y variable
google$facebook, #Z variable
xlab = "data_viz",
ylab = "degree",
zlab = "facebook",
col  = "red",
size = 3)
sn <- read.csv("social_network.csv", header = T)
View(sn)
hist(sn$Age)
hist(sn$Age,
col = colors()[c(552,254,26,65,22,309)],
main = "Ages of Respondents\nSocial Networking Survey of 202 Users",
xlab = "Age of Repondents")
hist(sn$Age,
col = colors()[c(552,254,"blueviolet",65,22,309)],
main = "Ages of Respondents\nSocial Networking Survey of 202 Users",
xlab = "Age of Repondents")
hist(sn$Age,
col = colors()[c(552,254,333,65,22,309)],
main = "Ages of Respondents\nSocial Networking Survey of 202 Users",
xlab = "Age of Repondents")
hist(sn$Age,
col = colors()[c(552,254,1,65,22,999)],
main = "Ages of Respondents\nSocial Networking Survey of 202 Users",
xlab = "Age of Repondents")
hist(sn$Age,
col = colors()[c(552,254,3,65,22,999)],
main = "Ages of Respondents\nSocial Networking Survey of 202 Users",
xlab = "Age of Repondents")
hist(sn$Age,
col = colors()[c(552,254,3,65,22,499)],
main = "Ages of Respondents\nSocial Networking Survey of 202 Users",
xlab = "Age of Repondents")
hist(sn$Age,
col = colors()[c(552,254,3,65,22,499)],
main = "Ages of Respondents\nSocial Networking Survey of 202 Users",
xlab = "Age of Repondents")
hist(sn$Age,
col = colors()[c(552,254,3,65,22,499,9999)],
main = "Ages of Respondents\nSocial Networking Survey of 202 Users",
xlab = "Age of Repondents")
hist(sn$Age,
col = colors()[c(552,254,3,65,22,499,999)],
main = "Ages of Respondents\nSocial Networking Survey of 202 Users",
xlab = "Age of Repondents")
hist(sn$Age,
col = colors()[c(552,254,3,65,22,499,99)],
main = "Ages of Respondents\nSocial Networking Survey of 202 Users",
xlab = "Age of Repondents")
boxplot(sn$Age,
col = colors() [18],
notch = T,
horizontal = T,
main = "Ages of Repondents\nSocial Networking Survey of 202 Respondents",
xlab = "Age of Respondents")
View(sn)
sn
table(sn$Site)
site.freq <- site.freq[order(site.freq, decreasing = T)]
site.freq <- table(sn$Site)
site.freq <- site.freq[order(site.freq, decreasing = T)]
site.freq
prop.table(site.freq) #proportions
round(prop.table(site.freq), 2)
rounded <- round(prop.table(site.freq), 2)
hist(rounded)
boxplot(rounded)
barplot(rounded)
site.freq
pie(site.freq)
pct <- round(site.freq/sum(site.freq)*100)
table(sn$Site)
table(sn$Site)
View(sn)
pct <- round(site.freq/sum(site.freq)*100)
lbls <- paste(sn$Site, pct)
pie(site.freq,
labels = lbls,
col = rainbow(length(lbls))
main = "Pie Chart")
pie(site.freq,
labels = lbls,
col = rainbow(length(lbls)),
main = "Pie Chart")
pct <- round(sn$Site/sum(sn$Site)*100)
lbls <- paste(sn$Site, pct)
pie(sn$Site,
labels = lbls,
col = rainbow(length(lbls)),
main = "Pie Chart")
lbls <- paste(sn$Site, pct)
lbls
site.freq <- table(sn$Site)
pct <- round(site.freq/sum(site.freq)*100)
lbls <- paste(site.freq, pct)
pie(site.freq,
labels = lbls,
col = rainbow(length(lbls)),
main = "Pie Chart")
lbls <- paste(names(site.freq), pct)
pie(site.freq,
labels = lbls,
col = rainbow(length(lbls)),
main = "Pie Chart")
rm(values)
rm(lbls, pct, rounded, site.freq)
summary(sn$Age)
boxplot(sn$Age,
col = colors() [18],
notch = T,
horizontal = T,
main = "Ages of Repondents\nSocial Networking Survey of 202 Respondents",
xlab = "Age of Respondents")
summary(sn)
View(sn)
scatter.hist(sn$Times)
plot(sn$Times)
fivenum(sn$Age) #very condenced view of the data
install.packages("psych")
library("psych")
describe(sn)
plot(sn$Times)
hist(sn$Times)
describe(sn$Times)
times.z <- scale(sn$Times) #to find z-scores
hist(times.z)
describe(times.z)
times.ln0 <- log(sn$Times) #to find log values
hist(times.ln0)
times.z <- scale(sn$Times) #to find z-scores
hist(times.z)
times.ln0 <- log(sn$Times) #to find log values
hist(times.ln0)
describe(times.ln0) #data comes out weird because you can do logs for 0.
times.ln1 <- log(sn$Times + 1) #a workaround to eliminate 0s.
hist(times.ln1)
describe(times.ln1)
times.rank <- rank(sn$Times) #ranking
hist(times.rank)
describe(times.rank)
#ties.method = c("average", "first", "random", "max", "min")
times.rankr <- rank(sn$Times, ties.method = "random")
hist(times.rankr)
describe(times.rankr)
n1 <- rnorm(1000000) #create 1 million random norml values
hist(n1)
n3 <- rnorm(100)
n3
n3 <- rnorm(100)
n3
n2 <- rnorm(1000000) #create another 1 million random normal values
hist(n2)
n.add <- n1+n2 #average scores across two variables
hist(n.add)
n.mult <- n1*n2 #multiple scores across two vatiables
hist(n.mult)
#calculate kurtosis for each distribution
kurtosi(n1)
kurtosi(n2)
kurtosi(n.add)
kurtosi(n.mult)
ncubes <- n1^3
hist(ncube)
hist(ncube)
hist(ncubes)
hist(n1)
kurtosi(ncubes)
hist(ncubes)
google <- read.csv("google_correlate.csv", header = T)
View(google)
names(google) #names of vatiables
str(google) #structure
View(google)
View(google)
pairs(~data_viz + degree + facebook + nba,
data = google,
pch = 20,
main = "Simple Scatterplot Matriz")
pairs.panels(google[c(3,7,4,5)], gap = 0) #no gap between panels
names(google)
viz.reg.dist <- split(google$data_viz, google$region)
boxplot(viz.reg.dist, col = "lavender") #boxplot by region
#barplot with means
viz.reg.mean <- sapply(viz.reg.dist, mean) #s-apply
barplot(viz.reg.mean,
col = "beige",
main = "Average Google Search Share of\n\"Data Visualization\" by Region of US")
abline(h = 0)
#barplot with means
viz.reg.mean <- sapply(viz.reg.dist, mean) #s-apply
barplot(viz.reg.mean,
col = "beige",
main = "Average Google Search Share of\n\"Data Visualization\" by Region of US")
abline(h = 0)
describeBy(google$data_viz, google$region)
google <- read.csv("google_correlate.csv", header = T)
names(google)
str(google)
plot(google$degree, google$data_viz) #plot for scatterplot
plot(google$degree, google$data_viz,
main = "Interest in Data Visualization Searches\nby Percentage of Pupulation with College Degrees",
xlab = "Population with College Degrees",
ylab = "Searches for \"Data Visualization\"",
pch = 20, # appearance of the points
col= "grey")
abline(lm(google$data_viz ~ google$degree), col="red")
# Lowess smoother line
lines(lowess(google$degree, google$data_viz), col="blue")
#Spinning 3D scatterplot
#Install and load rgl package
install.packages("rgl")
library("rgl")
plot3d(google$data_viz, #X variable
google$degree,   #Y variable
google$facebook, #Z variable
xlab = "data_viz",
ylab = "degree",
zlab = "facebook",
col  = "red",
size = 3)
plot3d(google$data_viz, #X variable
google$degree,   #Y variable
google$facebook, #Z variable
xlab = "data_viz",
ylab = "degree",
zlab = "facebook",
col  = "red",
size = 10)
plot3d(google$data_viz, #X variable
google$degree,   #Y variable
google$facebook, #Z variable
xlab = "data_viz",
ylab = "degree",
zlab = "facebook",
col  = "red",
size = 3)
plot3d(rnorm(1000), #X variable
rnorm(1000),   #Y variable
rnorm(1000), #Z variable
xlab = "data_viz",
ylab = "degree",
zlab = "facebook",
col  = "red",
size = 3)
plot3d(abs(rnorm(1000)), #X variable
rnorm(1000),   #Y variable
rnorm(1000), #Z variable
xlab = "data_viz",
ylab = "degree",
zlab = "facebook",
col  = "red",
size = 3)
plot3d(abs(rnorm(1000)), #X variable
abs(rnorm(1000)),   #Y variable
abs(rnorm(1000)), #Z variable
xlab = "data_viz",
ylab = "degree",
zlab = "facebook",
col  = "red",
size = 3)
plot3d(abs(rnorm(1000)), #X variable
abs(rnorm(1000)),   #Y variable
rnorm(1000), #Z variable
xlab = "data_viz",
ylab = "degree",
zlab = "facebook",
col  = "red",
size = 3)
plot3d(abs(rnorm(1000)), #X variable
abs(rnorm(1000)),   #Y variable
rnorm(10000), #Z variable
xlab = "data_viz",
ylab = "degree",
zlab = "facebook",
col  = "red",
size = 3)
plot3d(abs(rnorm(10000)), #X variable
abs(rnorm(10000)),   #Y variable
rnorm(10000), #Z variable
xlab = "data_viz",
ylab = "degree",
zlab = "facebook",
col  = "red",
size = 3)
plot3d(abs(rnorm(10000)), #X variable
rnorm(10000)),   #Y variable
rnorm(10000), #Z variable
xlab = "data_viz",
ylab = "degree",
zlab = "facebook",
col  = "red",
size = 3)
plot3d(abs(rnorm(10000)), #X variable
rnorm(10000),   #Y variable
rnorm(10000), #Z variable
xlab = "data_viz",
ylab = "degree",
zlab = "facebook",
col  = "red",
size = 3)
google <- read.csv("google_correlate.csv", header = T)
names(google)
names(google)
g.quant <- google[c(3, 7, 4, 5)]
#Correlation matrix for data frame
cor(g.quant)
#Test one pair of variables at a time
cor.test(g.quant$data_viz, g.quant$degree)
#Calculate p-value for entire matrix
install.packages("Hmisc")
library("Hmisc")
#Then need to convert g.quant from data frame to matrix to get
#correlation matrix and p-values.
rcorr(as.matrix(g.quant))
google <- read.csv("google_correlate.csv", header = T)
names(google)
names(google)
#We will be predicting interest in data visualization. This is how
#common that term is as a Google search term relative to other
#searches on a state by state basis.
reg <- lm(data_viz ~
degree + stats_ed + facebook + nba + has_nba + region,
data = google)
#We will be predicting interest in data visualization. This is how
#common that term is as a Google search term relative to other
#searches on a state by state basis.
reg1 <- lm(data_viz ~
degree + stats_ed + facebook + nba + has_nba + region,
data = google)
rm(reg)
summary(reg1)
sn <- read.csv("social_networks.csv", header = T)
sn <- read.csv("social_network.csv", header = T)
names(sn)
names(sn)
sn <- read.csv("social_network2.csv", header = T)
names(sn)
names(sn)
names(sn)
#Create contigency talble
sn.tab <- table(sn$Gender, sn$Site)
sn.tab
#If desired, we can also get marginal frequencies.
margin.table(sn.tab, 1) #Row marginal frequencies.
margin.table(sn.tab, 2) #Column marginal frequencies.
#Get cell, row, and column proportions %, rounding to two decimals.
round(prop.table(sn.tab), 2)    #cell %
round(prop.table(sn.tab, 1), 2) #row %
round(prop.table(sn.tab, 2), 2) #column %
# Chi-squared test
chisq.test(sn.tab)
google <- read.csv("google_correlate.csv", header = T)
names(google)
names(google)
#independent 2-group t-test
t.test(google$nba ~ google$has_nba)
google <- read.csv("google_correlate.csv", header = T)
names(google)
#One way ANOVA
anova1 <- aov(data_viz ~ region, data = google)
summary(anova1)
#Two way factorial design, two ways to do it:
#1st way:
anova2a <- aov(data_viz ~
region + stats_ed + region:stats_ed,
data = google)
summary(anova2a)
#2nd way:
anova2b <- aov(data_viz ~
region*stats_ed,
data = google)
summary(anova2b)
google <- read.csv("google_correlate.csv", header = T)
names(google)
#create data frame with only quantitative variables
g.quant <- google[c(3, 7, 4, 5)]
#Correlation matrix for data frame. Correlation goes from -1 to 1.
cor(g.quant)
#Test one pair of variables at a time
#Gives r, hypothesis test, and confidence interval.
cor.test(g.quant$data_viz, g.quant$degree)
#Calculate p-value for entire matrix
install.packages("Hmisc")
library("Hmisc")
#Then need to convert g.quant from data frame to matrix to get
#correlation matrix and p-values.
rcorr(as.matrix(g.quant))
library("Hmisc")
#Then need to convert g.quant from data frame to matrix to get
#correlation matrix and p-values.
rcorr(as.matrix(g.quant))
names(google)
#We will be predicting interest in data visualization. This is how
#common that term is as a Google search term relative to other
#searches on a state by state basis.
reg1 <- lm(data_viz ~ #lm means linear model
degree + stats_ed + facebook + nba + has_nba + region,
data = google)
summary(reg1)
